{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aa19635",
   "metadata": {},
   "source": [
    "# Prototype\n",
    "A Notebook to understand how to structure NCERT and other study material data into structured data for langchain.\n",
    "\n",
    "Objectives:\n",
    "- How to Sructure Study Material and NCERT Data\n",
    "- How to give long term memory to LLM to make sure the LLM can Answer from its past experiece too"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2d6a92",
   "metadata": {},
   "source": [
    "## Imports and Neccesary Variables\n",
    "We will need Langchain library to connect Our Knowledge base to the LLM.Tools like LLMChain and ConversationChain will provide short-term memory and links to other tools.We will use CoversationBufferWindowMemory wih k=5 to keep only last 5 interactions since the Model has a Long term memory now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c927e7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open AI API Key: ········\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain nltk bs4 requests fake_useragent\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.agents import load_tools, initialize_agent\n",
    "from langchain.chains import LLMChain, ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "\n",
    "from getpass import getpass\n",
    "\n",
    "OPENAI_API_KEY = getpass(\"Open AI API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2bbd0f",
   "metadata": {},
   "source": [
    "## Installing Various dataset of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9373e009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "df27f3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'living': 1.2628643221541278, 'organisms': 1.2628643221541278, 'certain': 2.120263536200091, 'common': 1.2628643221541278, 'characteristics': 1.2628643221541278, 'breathing': 1.2628643221541278, 'growing': 1.2628643221541278, 'requiring': 1.2628643221541278, 'nutrition': 1.2628643221541278, 'producing': 1.2628643221541278, 'offspring': 1.2628643221541278, 'responding': 1.2628643221541278, 'stimuli': 1.2628643221541278, 'etc': 1.2628643221541278, 'vital': 1.2628643221541278, 'processes': 2.120263536200091, 'maintain': 1.2628643221541278, 'homeostasis': 1.2628643221541278, 'proper': 1.2628643221541278, 'functioning': 1.2628643221541278, 'body': 1.2628643221541278, 'called': 1.2628643221541278, 'life': 1.2628643221541278}\n"
     ]
    }
   ],
   "source": [
    "from fake_useragent import UserAgent\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import *\n",
    "import requests\n",
    "import heapq\n",
    "import nltk\n",
    "import json\n",
    "import re\n",
    "\n",
    "s = requests.Session()\n",
    "s.faker = UserAgent()\n",
    "s.headers = {\n",
    "    'User-Agent': s.faker['chrome']\n",
    "}\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "import string\n",
    "\n",
    "# Function to extract tags from the content\n",
    "def extract_tags(content, n_tags=5):\n",
    "    # Tokenize the content into words\n",
    "    tokens = word_tokenize(content.lower())\n",
    "\n",
    "    # Remove stopwords from the tokens\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Calculate word frequencies\n",
    "    fdist = FreqDist(filtered_tokens)\n",
    "\n",
    "    # Extract the most common tokens as tags\n",
    "    tags = set([token for token, _ in fdist.most_common(n_tags) if token not in string.punctuation])\n",
    "    return tags\n",
    "\n",
    "import math\n",
    "\n",
    "def calculate_tfidf(content):\n",
    "    # Tokenize the content into words\n",
    "    tokens = word_tokenize(content.lower())\n",
    "\n",
    "    # Remove stopwords from the tokens\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words and token not in string.punctuation]\n",
    "\n",
    "    # Calculate term frequencies\n",
    "    term_frequencies = nltk.FreqDist(filtered_tokens)\n",
    "    max_frequency = max(term_frequencies.values())\n",
    "\n",
    "    # Calculate document frequency\n",
    "    document_frequency = nltk.FreqDist(filtered_tokens)\n",
    "\n",
    "    # Calculate TF-IDF scores\n",
    "    tfidf_scores = {}\n",
    "    for token in filtered_tokens:\n",
    "        tf = term_frequencies[token] / max_frequency\n",
    "        idf = math.log(len(filtered_tokens) / (1 + document_frequency[token]))\n",
    "        tfidf_scores[token] = tf * idf\n",
    "\n",
    "    return tfidf_scores\n",
    "\n",
    "# Example usage\n",
    "content = \"All living organisms have certain common characteristics such as breathing, growing, requiring nutrition, producing offspring, responding to stimuli, etc. There are certain vital processes that maintain homeostasis and proper functioning of the body, they are called life processes.\"\n",
    "\n",
    "tfidf_scores = calculate_tfidf(content)\n",
    "print(tfidf_scores)\n",
    "\n",
    "\n",
    "def summarize(article):\n",
    "    sentence_list = nltk.sent_tokenize(article.replace(\"\\n\",''))\n",
    "    stopwrds = stopwords.words(\"english\")\n",
    "    word_frequencies = {}\n",
    "    for word in nltk.word_tokenize(article):\n",
    "        if word not in stopwrds:\n",
    "            if word not in word_frequencies.keys():\n",
    "                word_frequencies[word] = 1\n",
    "            else:\n",
    "                word_frequencies[word] += 1\n",
    "\n",
    "    maximum_frequncy = max(word_frequencies.values())\n",
    "\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "\n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_list:\n",
    "        for word in nltk.word_tokenize(sent.lower()):\n",
    "            if word in word_frequencies.keys():\n",
    "                if len(sent.split(' ')) < 30:\n",
    "                    if sent not in sentence_scores.keys():\n",
    "                        sentence_scores[sent] = word_frequencies[word]\n",
    "                    else:\n",
    "                        sentence_scores[sent] += word_frequencies[word]\n",
    "\n",
    "    summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "    summary = ' '.join(summary_sentences)\n",
    "    return str(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6e4161",
   "metadata": {},
   "source": [
    "## Get the data and structure it\n",
    "Get data from various sources like NCERT books, and Online resources and structure it for the database.\n",
    "The structured data will be then uploaded to any SQL databse and then be used as a KnowledgeBase for the Chat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
